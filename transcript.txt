[MUSIC PLAING] DAVID MALAN: All right, this is CS50, Harvard University's introduction to the intellectual enterprises of computer science and the art of programming, back here on campus in beautiful Sanders Theatre for the first time in quite a while. So welcome to the class. My name is David-- OK. 

[CHEERING AND APPLAUSE] 

So my name is David Malan. And I took this class myself some time ago, but almost didn't. It was sophomore fall and I was sitting in on the class. And I was a little curious but, eh, it didn't really feel like the field for me. I was definitely a computer person, but computer science felt like something altogether. And I only got up the nerve to take the class, ultimately, because the professor at the time, Brian Kernighan, allowed me to take the class pass/fail, initially. And that is what made all the difference. 

I quickly found that computer science is not just about programming and working in isolation on your computer. It's really about problem solving more generally. And there was something about homework, frankly, that was, like, actually fun for perhaps the first time in, what, 19 years. And there was something about this ability that I discovered, along with all of my classmates, to actually create something and bring a computer to life to solve a problem, and sort of bring to bear something that I'd been using every day but didn't really know how to harness, that's been gratifying ever since, and definitely challenging and frustrating. 

Like, to this day, all these years later, you're going to run up against mistakes, otherwise known as bugs, in programming, that just drive you nuts. And you feel like you've hit a wall. But the trick really is to give it enough time, to take a step back, take a break when you need to. And there's nothing better, I daresay, than that sense of gratification and pride, really, when you get something to work, and in a class like this, present, ultimately, at term's end, something like your very own final project. 

Now, this isn't to say that I took to it 100% perfectly. In fact, just this past week, I looked in my old CS50 binder, which I still have from some 25 years ago, and took a photo of what was apparently the very first program that I wrote and submitted, and quickly received minus 2 points on. But this is a program that we'll soon see in the coming days that does something quite simply like print "Hello, CS50," in this case, to the screen. 

And to be fair, I technically hadn't really followed the directions, which is why I lost those couple of points. But if you just look at this, especially if you've never programmed before, you might have heard about programming language but you've never typed something like this out, undoubtedly it's going to look cryptic. But unlike human languages, frankly, which were a lot more sophisticated, a lot more vocabulary, a lot more grammatical rules, programming, once you start to wrap your mind around what it is and how it works and what these various languages are, it's so easy, you'll see, after a few months of a class like this, to start teaching yourself, subsequently, other languages, as they may come, in the coming years as well. 

So what ultimately matters in this particular course is not so much where you end up relative to your classmates but where you end up relative to yourself when you began. And indeed, you'll begin today. And the only experience that matters ultimately in this class is your own. And so, consider where you are today. Consider, perhaps, just how cryptic something like that looked a few seconds ago. And take comfort in knowing just some months from now all of that will be within your own grasp. 

And if you're thinking that, OK, surely the person in front of me, to the left, to the right, behind me, knows more than me, that's statistically not the case. 2/3 of CS50 students have never taken a CS course before, which is to say, you're in very good company throughout this whole term. 

So then, what is computer science? I claim that it's problem solving. And the upside of that is that problem solving is something we sort of do all the time. But a computer science class, learning to program, I think kind of cleans up your thoughts. It helps you learn how to think more methodically, more carefully, more correctly, more precisely. Because, honestly, the computer is not going to do what you want unless you are correct and precise and methodical. 

And so, as such, there's these fringe benefits of just learning to think like a computer scientist and a programmer. And it doesn't take all that much to start doing so. This, for instance, is perhaps the simplest picture of computer science, sure, but really problem solving in general. Problems are all about taking input, like the problem you want to solve. You want to get the solution, a.k.a. output. And so, something interesting has got to be happening in here, in here, when you're trying to get from those inputs to outputs. 

Now, in the world of computers specifically, we need to decide in advance how we represent these inputs and outputs. We all just need to decide, whether it's Macs or PCs or phones or something else, that we're all going to speak some common language, irrespective of our human languages as well. And you may very well know that computers tend to speak only what language, so to speak? 

Assembly, one, but binary, two, might be your go-to. And binary, by implying two, means that the world of computers has just two digits at its disposal, 0 and 1. And indeed, we humans have many more than that, certainly not just zeros and ones alone. But a computer indeed only has zeros and ones. And yet, somehow they can do so much. They can crunch numbers in Excel, send text messages, create images and artwork and movies and more. 

And so, how do you get from something as simple as a few zeros, a few ones, to all of the stuff that we're doing today in our pockets and laptops and desktops? Well, it turns out that we can start quite simply. If a computer were to want to do something as simple as count, well, what could it do? 

Well, in our human world, we might count doing this, like 1, 2, 3, 4, 5, using so-called unitary notation, literally the digits on your fingers where one finger represents one person in the room, if I'm, for instance, taking attendance. Now, we humans would typically actually count 1, 2, 3, 4, 5, 6. And we'd go past just those five digits and count much higher, using zeros through nines. But computers, somehow, only have these zeros and ones. 

So if a computer only somehow speaks binary, zeros and ones, how does it even count past the number 1? Well, here are 3 zeros, of course. And if you translate this number in binary, 000, to a more familiar number in decimal, we would just call this zero. Enough said. If we were to represent, with a computer, the number 1, it would actually be 001, which, not surprisingly, is exactly the same as we might do in our human world, but we might not bother writing out the two zeros at the beginning. 

But a computer, now, if it wants to count as high as two, it doesn't have the digit 2. And so it has to use a different pattern of zeros and ones. And that happens to be 010. So this is not 10 with a zero in front of it. It's indeed zero one zero in the context of binary. 

And if we want to count higher now than two, we're going to have to tweak these zeros and ones further to get 3. And then if we want 4 or 5 or 6 or 7, we're just kind of toggling these zeros and ones, a.k.a. bits, for binary digits that represent, via these different patterns, different numbers that you and I, as humans, know, of course, as the so-called decimal system, 0 through 9, dec implying 10, 10 digits, those zeros through nine. 

So why that particular pattern? And why these particular zeros and ones? Well, it turns out that representing one thing or the other is just really simple for a computer. Why? At the end of the day, they're powered by electricity. And it's a really simple thing to just either store some electricity or don't store some electricity. Like, that's as simple as the world can get, on or off. 1 or 0, so to speak. 

So, in fact, inside of a computer, a phone, anything these days that's electronic, pretty much, is some number of switches, otherwise known as transistors. And they're tiny. You've got thousands, millions of them in your Mac or PC or phone these days. And these are just tiny little switches that can get turned on and off. And by turning those things on and off in patterns, a computer can count from 0 on up to 7, and even higher than that. 

And so these switches, really, you can think of being as like switches like this. Let me just borrow one of our little stage lights here. Here's a light bulb. It's currently off. And so, I could just think of this as representing, in my laptop, a transistor, a switch, representing 0. But if I allow some electricity to flow, now I, in fact, have a 1. 

Well, how do I count higher than 1? I, of course, need another light bulb. So let me grab another one here. And if I put it in that same kind of pattern, I don't want to just do this. That's sort of the old finger counting way of unary, just 1, 2. I want to actually take into account the pattern of these things being on and off. So if this was one a moment ago, what I think I did earlier was I turned it off and let the next one over be on, a.k.a. 010. 

And let me get us a third bit, if you will. And that feels like enough. Here is that same pattern now, starting at the beginning with 3. So here is 000. Here is 001. Here is 010, a.k.a., in our human world of decimal, 2. And then we could, of course, keep counting further. This now would be 3 and dot dot dot. If this other bulb now goes on, and that switch is turned and all three stay on-- this, again, was what number? 

AUDIENCE: Seven. 

DAVID MALAN: OK, so, seven. So it's just as simple, relatively, as that, if you will. But how is it that these patterns came to be? Well, these patterns actually follow something very familiar. You and I don't really think about it at this level anymore because we've probably been doing math and numbers since grade school or whatnot. But if we consider something in decimal, like the number 123, I immediately jump to that. This looks like 123 in decimal. But why? 

It's really just three symbols, a 1, a 2 with a bit of curve, a 3 with a couple of curves, that you and I now instinctively just assign meaning to. But if we do rewind a few years, that is one hundred twenty-three because you're assigning meaning to each of these columns. The 3 is in the so-called ones place. The 2 is in the so-called tens place. And the 1 is in the so-called hundreds place. 

And then the math ensues quickly in your head. This is technically 100 times 1, plus 10 times 2, plus 1 times 3, a.k.a. 100 plus 20 plus 3. And there we get the sort of mathematical notion we know as 123. 

Well, nicely enough, in binary, it's actually the same thing. It's just these columns mean a little something different. If you use three digits in decimal, and you have the ones place, the tens place, and the hundreds place, well, why was that 1, 10, and 100? They're technically just powers of 10. So 10 to the 0, 10 to the 1, 10 to the 2. Why 10? Decimal system, "dec" meaning 10. You have 8 and 10 digits, 0 through 9. 

In the binary system, if you're going to use three digits, just change the bases if you're using only zeros and ones. So now it's powers of 2, 2 to the 0, 2 to the 1, 2 to the 2, a.k.a. 1 and 2 and 4, respectively. And if you keep going, it's going to be 8s column, 16s column, 32, 64, and so forth. 

So, why did we get these patterns that we did? Here's your 000 because it's 4 times 0, 2 times 0, 1 times 0, obviously 0. This is why we got the decimal number 1 in binary. This is why we got the number 2 in binary, because it's 4 times 0, plus 2 times 1, plus 1 times 0, and now 3, and now 4, and now 5, and now 6, and now 7. And, of course, if you wanted to count as high as 8, to be clear, what do you have to do? What does a computer need to do to count even higher than 7? 